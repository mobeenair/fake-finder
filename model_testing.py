# -*- coding: utf-8 -*-
"""FINAL model testing, tweets, text and test_set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MkrUnRhgdI4B13vF1bEKH8UOVm9aTWxy

# Step 1: Import required libraries
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import os
import pickle
from tensorflow.keras.models import load_model

import tweepy

"""# Step 2: Connect with google drive
Make sure to upload the data to be used on your google drive account
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Step 3: Get tweets from twitter"""

# CONNECT to twitter API

CONSUMER_KEY = 'Eh0sTdxflRe7BEW8lYc44RFI9'
CONSUMER_KEY_SECRET = 'J07smZmzHp04WrSS8n4pMMtenqqJ6IhlKIJjJWzJKw8xLoxG0s'
ACCESS_TOKEN = '2860698043-ITWJ47MzDIGiPDBST5cjda57Yu5xUVJZDOg7Hkf'
ACCESS_TOKEN_SECRET= 'k3l99VdPPlOKKyGlE0TLBjBYtLkbh5etqqg8XpmREVgQi'

auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_KEY_SECRET)
auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)

api = tweepy.API(auth)

# OPTION 1. searches for the query in twitter. Can search for hashtags, keywords easily and also filter retweets to analyze the original text only.
search_query = 'BTS UNO ON SOBA' +' -filter:retweets'
tweets = api.search(search_query, count=2000) 
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])

display(data.head(10))

# OPTION 2. Get tweets from a user timeline 
userID = "realDonaldTrump"
tweets = api.user_timeline(screen_name=userID, count=2000, include_rts = False, tweet_mode = 'extended', lang="en")

tweets_list = []
for info in tweets:
     #print("ID: {}".format(info.id))
     #print(info.created_at)
     #print(info.full_text)
     #print("\n")
     tweets_list.append(info.full_text)

data = pd.DataFrame(data=[tweet for tweet in tweets_list], columns=['Tweets'])
display(data.head(10))

"""# Step 4: Preprocess data according to GloVe suggestion"""

# PREPROCESSING acc to glove

import regex as re

FLAGS = re.MULTILINE | re.DOTALL

def hashtag(text):
  text = text.group()
  hashtag_body = text[1:]
  if hashtag_body.isupper():
    result = "<hashtag> {} <allcaps>".format(hashtag_body.lower())
  else:
    result = " ".join(["<hashtag>"] + re.split(r"(?=[A-Z])", hashtag_body, flags=FLAGS))
  return result

def allcaps(text):
  text = text.group()
  return text.lower() + " <allcaps> " # amackcrane added trailing space


def clean_text(text):
  # Different regex parts for smiley faces
  eyes = r"[8:=;]"
  nose = r"['`\-]?"

  # function so code less repetitive
  def re_sub(pattern, repl):
    return re.sub(pattern, repl, text, flags=FLAGS)

  text = re_sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", "<url>")
  text = re_sub(r"@\w+", "<user>")

  text = re_sub(r"{}{}[)dD]+|[)dD]+{}{}".format(eyes, nose, nose, eyes), "<smile>")
  text = re_sub(r"{}{}p+".format(eyes, nose), "<lolface>")
  text = re_sub(r"{}{}\(+|\)+{}{}".format(eyes, nose, nose, eyes), "<sadface>")
  text = re_sub(r"{}{}[\/|l*]".format(eyes, nose), "<neutralface>")
  text = re_sub(r"/"," / ")
  text = re_sub(r"<3","<heart>")

  text = re_sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", "<number>")
  text = re_sub(r"#\w+", hashtag)  
  text = re_sub(r"([!?.]){2,}", r"\1 <repeat>")
  text = re_sub(r"\b(\S*?)(.)\2{2,}\b", r"\1\2 <elong>")

  text = re_sub(r"([a-zA-Z<>()])([?!.:;,])", r"\1 \2")
  text = re_sub(r"\(([a-zA-Z<>]+)\)", r"( \1 )")
  text = re_sub(r"  ", r" ")

  text = re_sub(r" ([A-Z]){2,} ", allcaps)
  text = text.lower()
  return text

data= data.dropna(subset=['Tweets'])
data['Tweets'] = data['Tweets'].apply(clean_text)
data.head(10)

data.shape

"""# Step 5: Load H5 Model for Classification"""

# LOAD MODEL...
load_path = "/content/drive/My Drive/Colab Notebooks/"
loaded_model = load_model(os.path.join(load_path,"tpu_lstm.h5"))
loaded_model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
loaded_model.summary()

"""## **STEP 6: Test MODEL**"""

# LOADING tokenizer and then convert the new tweet in texts_to_sequence

MAX_SEQUENCE_LENGTH = 32
MAX_NB_WORDS = 30000      # MAX_NB_WORDS is size of corpus (based on training data)
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)

TOKENIZER_DIR = "/content/drive/My Drive/Colab Notebooks/Datasets/"
# load tokenizer
with open(os.path.join(TOKENIZER_DIR, 'tokenizer.pickle'), 'rb') as handle:
  tokenizer = pickle.load(handle)

# Pridicting bot or genuine(human) bahviour of tweets from the model

labels = ['bot','geniune']
bot_count = 0
gen_count = 0
seq = tokenizer.texts_to_sequences(data["Tweets"].values)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)

rows = padded.shape[0]
for n in range(0, rows):
  X = np.array(padded[n])     # was (32,) column
  X_trans = X.reshape((1,32)) # reshape to row vector

  pred = loaded_model.predict(X_trans) # prediction between 0 and 1 bot and human
  print(pred, labels)
  output='{0:.{1}f}'.format(pred[0][0], 4) # pred[0][0] has bot probability...
  print(output)
  # if probability of being bot is less than 0.5
  if(output>str(0.5)):
    bot_count+=1
    print("BOT COUNT INCRESED TO ", bot_count)
  else:
    gen_count+=1 
    print("GEN COUNT INCRESED TO ", gen_count)

print("Out of ", gen_count+bot_count, "Bot counts=", bot_count, "Gen counts=", gen_count)

# chekcing the loaded_model with new tweets/text...

new_tweet = "@drex What kind of surgery did you have? Had a root canal done last year and it wasn't fun during the aftermath."
clean_tweet = clean_text(new_tweet) 
print(clean_tweet)

seq = tokenizer.texts_to_sequences([clean_tweet])
print(seq)

padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
print(padded)

pred = loaded_model.predict(np.array(padded))
labels = ['bot','geniune']
print(pred, labels)
print(labels[np.argmax(pred)])

print(pred[0][1])

output='{0:.{1}f}'.format(pred[0][0], 4)
print(output)

"""# Test Model from testing data"""

# load testing data...
df_Y_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Cleaned Training and Testing Datasets/Y_test.csv')
df_X_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Cleaned Training and Testing Datasets/X_test.csv')

Y_test = df_Y_test.to_numpy()
X_test = df_X_test.to_numpy()

#confusion Matrix to LSTM-Classifier

from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report

y_pred=loaded_model.predict_classes(X_test) # predict_classes used the round variable values instead of points

tests=np.argmax(Y_test, axis=1) # convert the decimal points to rounded 

# we provide the confusion matrix a rounded value otherwise a classification matrix problem arieses
cm=confusion_matrix(y_pred, tests)


target_names = ['bot', 'genuine']

print(cm)
print('Accuracy Score :',accuracy_score(tests, y_pred)) 
print('Report : ') 
print(classification_report(tests, y_pred, target_names=target_names))

# THIS IS FOR TESTING ONLY 
# Pridicting bot or genuine(human) bahviour of tweets from the model

labels = ['bot','geniune']
seq = tokenizer.texts_to_sequences(data["Tweets"].values)
print(seq)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
print(padded, 'padded shape of rows is {}', padded.shape[0])

rows = padded.shape[0]

for n in range(0, rows):
  #print(X)
  X = np.array(padded[n])
  print(X)
  X_trans = X.reshape((1,32))
  print(X_trans)
  pred = loaded_model.predict(X_trans)
  print(pred, labels)
  output='{0:.{1}f}'.format(pred[0][0], 4)
  print(output)

# for i in range(len(data)) : 
#   print(data.loc[i, "Tweets"])

# Max number of words in each tweet.
#MAX_SEQUENCE_LENGTH = 32
#tokenizer = Tokenizer(num_words=30000) 
tokenizer.fit_on_texts(data['Tweets'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(data['Tweets'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) # do this step without maxlen and it will pad by the max length it finds in data
print('Shape of data tensor:', X.shape)

"""# Test Model"""