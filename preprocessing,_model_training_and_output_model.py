# -*- coding: utf-8 -*-
"""Preprocessing, Model Training and Model output.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ybf0pNuv3_xBM0-BkrWxwKuVdlxmzDG0

# Step 1: Import required libraries
"""

import numpy as np 
import pandas as pd 
from pandas import DataFrame
import matplotlib.pyplot as plt

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
import tensorflow as tf
print(tf.__version__)

import regex as re
from keras.layers import Dense, Embedding, LSTM, Bidirectional
from sklearn.model_selection import train_test_split

from keras.callbacks import EarlyStopping
from keras.layers import Dropout

import io
import os
from tensorflow.keras.models import load_model

"""# Step 2: Connect with google drive
Make sure to upload the data to be used on your google drive account
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Step 3: Load dataset into dataframe"""

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Datasets/spbot1_spbot2_gen_500k.csv', dtype={"text":str, "account_type":str})
df.info()

df.account_type.value_counts()
df= df.dropna(subset=['text'])
df.shape

df.head(15)

"""# Step 4: Preprocess data according to GloVe suggestion"""

# preprocessing acc. to glove


FLAGS = re.MULTILINE | re.DOTALL

def hashtag(text):
  text = text.group()
  hashtag_body = text[1:]
  if hashtag_body.isupper():
    result = "<hashtag> {} <allcaps>".format(hashtag_body.lower())
  else:
    result = " ".join(["<hashtag>"] + re.split(r"(?=[A-Z])", hashtag_body, flags=FLAGS))
  return result

def allcaps(text):
  text = text.group()
  return text.lower() + " <allcaps> " # amackcrane added trailing space


def clean_text(text):
  # Different regex parts for smiley faces
  eyes = r"[8:=;]"
  nose = r"['`\-]?"

  # function so code less repetitive
  def re_sub(pattern, repl):
    return re.sub(pattern, repl, text, flags=FLAGS)

  text = re_sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", "<url>")
  text = re_sub(r"@\w+", "<user>")

  text = re_sub(r"{}{}[)dD]+|[)dD]+{}{}".format(eyes, nose, nose, eyes), "<smile>")
  text = re_sub(r"{}{}p+".format(eyes, nose), "<lolface>")
  text = re_sub(r"{}{}\(+|\)+{}{}".format(eyes, nose, nose, eyes), "<sadface>")
  text = re_sub(r"{}{}[\/|l*]".format(eyes, nose), "<neutralface>")
  text = re_sub(r"/"," / ")
  text = re_sub(r"<3","<heart>")

  text = re_sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", "<number>")
  text = re_sub(r"#\w+", hashtag)  
  text = re_sub(r"([!?.]){2,}", r"\1 <repeat>")
  text = re_sub(r"\b(\S*?)(.)\2{2,}\b", r"\1\2 <elong>")

  text = re_sub(r"([a-zA-Z<>()])([?!.:;,])", r"\1 \2")
  text = re_sub(r"\(([a-zA-Z<>]+)\)", r"( \1 )")
  text = re_sub(r"  ", r" ")

  text = re_sub(r" ([A-Z]){2,} ", allcaps)
  text = text.lower()
  return text


df['text'] = df['text'].apply(clean_text)
df.head(10)

"""# Step 5: Tokenize"""

# hyper parameters used 
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 30000
# Max number of words in each tweet.
MAX_SEQUENCE_LENGTH = 32

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(df['text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(df['text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) # do this step without maxlen and it will pad by the max length it finds in data
print('Shape of data tensor:', X.shape)

#Converting categorical labels to numbers.
Y = pd.get_dummies(df['account_type']).values
print('Shape of label tensor:', Y.shape)

"""# Step 6: Training and Testing data split"""

#Train test split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

# pre trained GloVe word embeddings
# Next, we compute an index, mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:

GLOVE_DIR = "/content/drive/My Drive/Colab Notebooks/Datasets/"

embeddings_index = {}

f = open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.50d.txt'))
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

# hyper paramters 
EMBEDDING_DIM = 50
epochs = 15
batch_size = 32

lstm_dim = 32

# At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:

embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))
counter = 0
for word, i in word_index.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    # words not found in embedding index will be all-zeros.
    embedding_matrix[i-1] = embedding_vector
  counter += 1
  if counter == MAX_NB_WORDS:
    break

embedding_matrix.shape

"""# Step 7: Training"""

# new model according to the paper...

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=X.shape[1]))

model.add(Bidirectional(LSTM(units=lstm_dim,dropout=0.1)))

model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))

model.add(Dense(2, activation="sigmoid"))
model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
model.summary()

# RUN THE MODEL...
history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.11,callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)])

# Final evaluation of the model
scores = model.evaluate(X_test, Y_test, verbose=1)
print("Accuracy: %.2f%%" % (scores[1]*100))

# accr = model.evaluate(X_test,Y_test)
# print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

# SAVE MODEL...
save_path = "/content/drive/My Drive/Colab Notebooks/"
# save entire network to HDF5 (save everything, suggested)
model.save(os.path.join(save_path,"model_name.h5"))
model.save_weights(os.path.join(save_path,"model_name_weights.h5"))

# LOAD MODEL...
load_path = "/content/drive/My Drive/Colab Notebooks/"
loaded_model = load_model(os.path.join(load_path,"tpu_lstm.h5"))
loaded_model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
loaded_model.summary()

#confusion Matrix to LSTM-Classifier

from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 

y_pred=loaded_model.predict_classes(X_test) # predict_classes used the round variable values instead of points

tests=np.argmax(Y_test, axis=1) # convert the decimal points to rounded 

# we provide the confusion matrix a rounded value otherwise a classification matrix problem arieses
cm=confusion_matrix(y_pred, tests)

target_names = ['bot', 'genuine']

print(cm)
print('Accuracy Score :',accuracy_score(tests, y_pred)) 
print('Report : ') 
print(classification_report(tests, y_pred, target_names=target_names))

# chekcing the loaded_model with new tweets/text...

new_tweet = "@drex What kind of surgery did you have? Had a root canal done last year and it wasn't fun during the aftermath."
clean_tweet = clean_text(new_tweet) 
print(clean_tweet)

seq = tokenizer.texts_to_sequences([clean_tweet])
print(seq)

padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
print(padded)

pred = loaded_model.predict(np.array(padded))
labels = ['bot','geniune']
print(pred, labels)
print(labels[np.argmax(pred)])

print(pred[0][1])

output='{0:.{1}f}'.format(pred[0][0], 4)
print(output)